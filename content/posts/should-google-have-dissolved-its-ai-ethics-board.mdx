---
  title: "Should Google Have Dissolved Its Ai Ethics Board?"
  author: "Jason Vlajankov"
  date: 2022-07-05
  tags: ["informationtechnology", "ethics"]
---

# Background 

Artificial Intelligence today is defined as the study and design of rational agents. An agent of rationality can be described as "one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome" (Russell, Norvig, and Davis (2010), p. 4 ). These rational agents are capable of performing tasks that would require or exceed human intelligence, and typically at much faster speeds due to their digital minds, and access to expanded hardware.

In 2018, Google published their "Google AI Principles"; a list of general objectives and prohibitions for their AI development that they adhere by (Google (2018)). Shortly after, Google introduced their Advanced Technology External Advisory Council (ATEAC), tasked with considering "some of Google's most complex challenges that arise under our AI Principles, like facial recognition and fairness in machine learning, providing diverse perspectives to inform our work" (Google (2019)).

One week after launching however, it began having troubles due to outcry regarding the choice of some members on the board, resulting in an open letter signed by over 2500 people including Google employees, professors, directors, etc (Googlers Against Transphobia (2019)). Google resigned itself and decided to "go back to the drawing board", finally dissolving the council (Levin (2019)). This briefing paper will evaluate how this was a poor ethical decision by Google, and the recommended steps they should take henceforth.


# Key Concepts 

With the introduction of new technology, comes a myriad of new ethical issues alongside it. The risk of AI on privacy and data protection; "creating new data protection risks not envisaged by legislation and thereby create new ethical concerns" (Stahl (2021), p. 41). Furthermore, there is a big risk of AI being used for war or national security, in that it can offer military superiority, information superiority, and economic superiority (Allen and Chan (2017)). Early indications of this may already be happening, as Google has partnered with the U.S. Department of Defense to help analyse drone footage in order to improve computer vision (Vincent (2018)).

An even more pressing theory, although much further in the future, is the concept of the "intelligence explosion", coined by mathematician Irving John Good, referring to the wake of an ultra-intelligent AI (surpassing all human capabilities), which continues to develop more and more intelligent machines (Good (1966)). A survey of experts indicate that most believe there is a 50% chance of such a high-level machine intelligence being developed around 2040-2050, increasing to a 90% chance by 2075, additionally expecting these systems to move on to "superintelligence" less than 30 years after (MÃ¼ller and Bostrom (2016), p. 1).

This concept of "Superintelligence" was expanded upon by Swedish philosopher Nick Bostrom, who claims that such an explosion is inevitable; "the chances that we will all find the sense to put down the dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite button just to see what happens." (Bostrom (2014), p. 259). Bostrom describes how this superintelligence may lead to disastrous consequences: "We cannot blithely assume that a superintelligence with a final goal of calculating decimals of pi (or making paperclips, or counting grains of sand) would limit its activities in such a way as not to infringe on human interests" (Bostrom (2014), p. 107). I have established that there exists a number of ethical issues both in modern day AI and in future, more advanced AI.

Such discussed ethical issues including national security, privacy, and superintelligent agents invariably concern the broad public, making them a major stakeholder in the discussion of AI ethics. After the dissolution of Google's AI ethics advisory council, the only guidelines for Google's use of AI are their published AI principles, which, they have stated, need some help considering some of the more complex challenges (Google (2019)). Given the risk associated with the development of AI, these algorithms should be carefully managed, hopefully ensuring that these challenges are met ethically. The public thinks so too; as a survey conducted by Zhang and Dafoe (2019) indicates that the overwhelming majority of Americans (82%) believe this should be the case; a figure comparable to another survey conducted with EU respondents ((2019), p. 10). Zhang and Dafoe (2019) go on to state that the four most important issues highlighted by respondents were: 1) Preventing violation of liberties from AI-assisted surveillance; 2) Preventing AI from spreading fake and harmful information; 3) Preventing AI cyber attacks; and 4) Protecting data privacy ((2019), pp. 3-4).

I have established that the public are one of the major secondary stakeholders (in all other scenarios except superintelligences, in which then they are likely instead primary stakeholders) for Google's ethical AI board and their demand for the careful management of the development and use of AI exists. Otherwise, primary stakeholders include Google (the company directly responsible), Google executives/employees (they work for Google and are manufacturing AI products/services), and users of Google services (as their data is in use when training to improve AI). Government agencies and other companies may be either primary or secondary stakeholders, contingent on whether they are customers of Google or directly impact development/use of AI products (for example, in national security/war scenarios; see Vincent (2018), and Allen and Chan (2017)).


# Utilitarianism

To evaluate the goodness of Google's decision to dissolve their AI ethics board, and to provide ethical recommendations, I will be analysing the scenario using the normative principle Utilitarianism, which evaluates the goodness of actions based on their utility (Duignan and West (2000)). In other words, evaluating the goodness of actions based on their "aggregate welfare"; evaluating the net benefit over the net harm (Hooker (2016)).

One of the major oppositions to the AI ethics board was the inclusion of a controversial member with disagreeable opinions regarding transgender people, LGBTQ people, and immigrants (Googlers Against Transphobia (2019)). Nonpartisan American think tank "Pew Research Center" describes that even for homosexuality, there exists a global divide that, although making progress in acceptance, still persists today (Poushter and Kent (n.d.)). Like in politics, compromise in an AI ethics council may be necessary to: 1) include all stakeholder voices; 2) progress the field; and, 3) to provide advice to Google for decisions as it pertains to their conduct of AI.

In a scenario where the AI ethics board were never dissolved, further strides in AI ethics would have been made than what will now be made because of the dissolution of it. While doing this, the board can then further vote to make internal changes regarding members, structure, decisions made, etc. Thus, although the foundations of the board may be unideal, the overall net benefit that it provides to Google, the broad public, other stakeholders and to AI ethics as a whole will become a net benefit to utility. However, as a result of the decision made, this hypothetical net benefit to utility was never realised, making this a poor decision by Google.


# Ethical Recommendation 

Given the previous utilitarian assessment, the first ethical recommendation to be made is for Google to revive their AI ethics board. However, to enable the board to perform better ethics work, and as the public puts more trust into expert opinion from university researchers (Zhang and Dafoe (2019)), the members of the board should not be individuals from differing fields or groups, but from experts in the fields of AI, ethics and specialised AI ethics; professional ethicists and alike. To deal with a lack of minority or expert opinion (from other fields), the AI ethics board should seek input from qualified individuals or adjacent councils, however they should not be directly involved with decision making.

The second ethical recommendation to be made is that the ethics AI board follows academic literature for AI ethics, adhering to widely documented important key principles of AI development when advising Google on their conduct. In particular, these key principles include transparency, predictability, and robustness against manipulation (Bostrom, Yudkowsky (2019), p. 58). Much of the global landscape agrees with such fundamental principles of transparency, justice and fairness, non-maleficence, responsibility and privacy (Jobin, Ienca, and Vayena (2019)). Expanding on this, the ethics AI board should go on further to review the meaning behind these principles of AI ethics guidelines, and how they relate to Google's development and deployment of AI tools.


# Conclusion 

This briefing paper has demonstrated how Google's decision to dissolve their AI ethics board was incorrect, citing evidence including surveys from the public and experts, peer-reviewed academic literature of AI and AI ethics, as well as journalistic sources from mainstream news publications. Using such evidence, critical analysis was provided on the goodness of the decision using the ethical principle Utilitarianism, demonstrating how net utility was lost due to the incorrect decision made. Finally, the paper recommended that Google revived the AI ethics board, additionally giving further instructions on specifically how to do so, as well as detailing some of the structure and conduct the board should take henceforth.


# References 

-   Allen, Chan (2017) Artificial Intelligence and National Security, National Security.
-   Bostrom (2014) Superintelligence: Paths, Dangers, Strategies, Oxford University Press.
-   Bostrom and Yudkowsky (2019) "The Ethics of Artificial Intelligence", pp. 57-58, in Yampolskiy (eds), _Artificial Intelligence Safety and Security_, CRC Press/Taylor &amp; Francis Group.
-   Duignan, and West, 2000, _Utilitarianism_, Encyclopedia Britannica, viewed 1st July 2022, https://www.britannica.com/topic/utilitarianism-philosophy
-   Good (1966) Speculations Concerning the First Ultraintelligent Machine\*\*Based on Talks given in a Conference on the Conceptual Aspects of Biocommunications, Neuropsychiatric Institute, University of California, Los Angeles, October 1962; and in the Artificial Intelligence Sessions of the Winter General Meetings of the IEEE, January 1963 [1, 46].The First Draft of This Monograph Was Completed in April 1963, and the Present Slightly Amended Version in May 1964.I Am Much Indebted to Mrs. Euthie Anthony of IDA for the Arduous Task of Typing., Elsevier.
-   Google (2018) Our Principles, Google AI.
-   Google (2019) An External Advisory Council to Help Advance the Responsible Development of AI, Google.
-   Googlers Against Transphobia (2019) Googlers Against Transphobia and Hate, Medium.
-   Hooker (2016) Rule Consequentialism, Metaphysics Research Lab, Stanford University.
-   Jobin, Ienca, Vayena (2019) The Global Landscape of AI Ethics Guidelines, Nature Machine Intelligence.
-   Levin (2019) Google Scraps AI Ethics Council after Backlash: 'Back to the Drawing Board', The Guardian.
-   Muller, Bostrom (2016) Future Progress in Artificial Intelligence: A Survey of Expert Opinion, Springer International Publishing.
-   Poushter, Kent (2020), The Global Divide on Homosexuality Persists, Pew Research Group.
-   Russell, Norvig, Davis (2010) Artificial Intelligence: A Modern Approach, Prentice Hall.
-   Stahl (2021) Ethical Issues of AI, Springer International Publishing.
-   Sullivan (2016) FAQ: All about the Google RankBrain Algorithm, Search Engine Land.
-   Vincent (2018) Google Is Using Its AI Skills to Help the Pentagon Learn to Analyze Drone Footage, The Verge.
-   Zhang, Dafoe (2019) Artificial Intelligence: American Attitudes and Trends, SSRN Electronic Journal.
